# -*- coding: utf-8 -*-
"""Pumps Maintenance.pynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eMp9QYstaNa1ag-CNvjTUedIUjNgkbMT
"""

import pandas as pd

# In Google Colab, if you've uploaded the file directly, it will be in the
# content directory, so just the filename is usually sufficient.
file_name = 'Large_Industrial_Pump_Maintenance_Dataset.csv'

try:
    # Load the CSV file into a Pandas DataFrame
    df = pd.read_csv(file_name)

    print("Successfully loaded the dataset!")

    # Display the first 5 rows of the DataFrame
    print("\nFirst 5 rows of the dataset:")
    print(df.head())

    # Display basic information about the DataFrame (columns, non-null counts, dtypes)
    print("\nDataFrame Info (Columns and Data Types):")
    df.info() # No need for print() as .info() is a method that prints directly

    # Display basic descriptive statistics for numerical columns
    print("\nBasic Descriptive Statistics:")
    print(df.describe())

except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found.")
    print("Please ensure the CSV file is uploaded to your Colab environment's session storage.")
    print("You can upload it by clicking the folder icon on the left sidebar, then the 'upload' icon.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'df' is already loaded from the previous step in your Colab notebook
# If you are running this in a new Colab session, you'll need to re-upload and re-run the loading script first:
# file_name = 'Large_Industrial_Pump_Maintenance_Dataset.csv'
# df = pd.read_csv(file_name)


print("Generating visualizations...")

# Set a style for the plots for better aesthetics
sns.set_style("whitegrid")
plt.figure(figsize=(18, 15)) # Adjust figure size for better readability of multiple plots

# --- 1. Vibration vs. Temperature (Scatter Plot) ---
plt.subplot(3, 2, 1) # 3 rows, 2 columns, 1st plot
sns.scatterplot(x='Temperature', y='Vibration', data=df, alpha=0.6, hue='Maintenance_Flag', palette='viridis')
plt.title('Vibration vs. Temperature')
plt.xlabel('Temperature')
plt.ylabel('Vibration')

# --- 2. Vibration vs. Pressure (Scatter Plot) ---
plt.subplot(3, 2, 2) # 3 rows, 2 columns, 2nd plot
sns.scatterplot(x='Pressure', y='Vibration', data=df, alpha=0.6, hue='Maintenance_Flag', palette='magma')
plt.title('Vibration vs. Pressure')
plt.xlabel('Pressure')
plt.ylabel('Vibration')

# --- 3. Vibration vs. Flow Rate (Scatter Plot) ---
plt.subplot(3, 2, 3) # 3 rows, 2 columns, 3rd plot
sns.scatterplot(x='Flow_Rate', y='Vibration', data=df, alpha=0.6, hue='Maintenance_Flag', palette='cividis')
plt.title('Vibration vs. Flow Rate')
plt.xlabel('Flow Rate')
plt.ylabel('Vibration')

# --- 4. Vibration vs. Operational Hours (Scatter Plot) ---
plt.subplot(3, 2, 4) # 3 rows, 2 columns, 4th plot
sns.scatterplot(x='Operational_Hours', y='Vibration', data=df, alpha=0.6, hue='Maintenance_Flag', palette='plasma')
plt.title('Vibration vs. Operational Hours')
plt.xlabel('Operational Hours')
plt.ylabel('Vibration')

# --- 5. Maintenance_Flag vs. Operational_Hours (Box Plot) ---
# Using a Box Plot as it's clear for comparing distributions between categories
plt.subplot(3, 2, 5) # 3 rows, 2 columns, 5th plot
sns.boxplot(x='Maintenance_Flag', y='Operational_Hours', data=df, palette='Set2')
plt.title('Operational Hours by Maintenance Flag')
plt.xlabel('Maintenance Flag (0: No Maintenance, 1: Maintenance)')
plt.ylabel('Operational Hours')

# Adjust layout to prevent overlapping titles/labels
plt.tight_layout()

# Display the plots
plt.show()

print("\nVisualizations generated successfully!")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'df' is already loaded from the previous step in your Colab notebook
# If you are running this in a new Colab session, you'll need to re-upload and re-run the loading script first:
# file_name = 'Large_Industrial_Pump_Maintenance_Dataset.csv'
# df = pd.read_csv(file_name)


print("Re-generating visualizations for better insights...")

# Set a style for the plots for better aesthetics
sns.set_style("whitegrid")
plt.figure(figsize=(20, 18)) # Increased figure size

# --- 1. Density Plot for Vibration by Maintenance Flag ---
plt.subplot(3, 2, 1) # 3 rows, 2 columns, 1st plot
sns.histplot(data=df, x='Vibration', hue='Maintenance_Flag', kde=True, stat='density', common_norm=False, palette='viridis')
plt.title('Distribution of Vibration by Maintenance Flag')
plt.xlabel('Vibration')
plt.ylabel('Density')

# --- 2. Density Plot for Temperature by Maintenance Flag ---
plt.subplot(3, 2, 2) # 3 rows, 2 columns, 2nd plot
sns.histplot(data=df, x='Temperature', hue='Maintenance_Flag', kde=True, stat='density', common_norm=False, palette='magma')
plt.title('Distribution of Temperature by Maintenance Flag')
plt.xlabel('Temperature')
plt.ylabel('Density')

# --- 3. Density Plot for Pressure by Maintenance Flag ---
plt.subplot(3, 2, 3) # 3 rows, 2 columns, 3rd plot
sns.histplot(data=df, x='Pressure', hue='Maintenance_Flag', kde=True, stat='density', common_norm=False, palette='cividis')
plt.title('Distribution of Pressure by Maintenance Flag')
plt.xlabel('Pressure')
plt.ylabel('Density')

# --- 4. Density Plot for Flow Rate by Maintenance Flag ---
plt.subplot(3, 2, 4) # 3 rows, 2 columns, 4th plot
sns.histplot(data=df, x='Flow_Rate', hue='Maintenance_Flag', kde=True, stat='density', common_norm=False, palette='plasma')
plt.title('Distribution of Flow Rate by Maintenance Flag')
plt.xlabel('Flow Rate')
plt.ylabel('Density')

# --- 5. Maintenance_Flag vs. Operational_Hours (Box Plot - This one was already good) ---
plt.subplot(3, 2, 5) # 3 rows, 2 columns, 5th plot
sns.boxplot(x='Maintenance_Flag', y='Operational_Hours', data=df, palette='Set2')
plt.title('Operational Hours by Maintenance Flag')
plt.xlabel('Maintenance Flag (0: No Maintenance, 1: Maintenance)')
plt.ylabel('Operational Hours')

# Adjust layout to prevent overlapping titles/labels
plt.tight_layout()

# Display the plots
plt.show()

print("\nVisualizations re-generated successfully! Please review the density plots for insights.")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'df' is already loaded from the previous step in your Colab notebook
# If you are running this in a new Colab session, you'll need to re-upload and re-run the loading script first:
# file_name = 'Large_Industrial_Pump_Maintenance_Dataset.csv'
# df = pd.read_csv(file_name)


print("Generating Box Plot for Operational Hours by Maintenance Flag...")

plt.figure(figsize=(8, 6)) # A smaller, focused figure
sns.boxplot(x='Maintenance_Flag', y='Operational_Hours', data=df, palette='Set2')
plt.title('Operational Hours by Maintenance Flag')
plt.xlabel('Maintenance Flag (0: No Maintenance, 1: Maintenance)')
plt.ylabel('Operational Hours')
plt.show()

print("\nBox plot generated successfully! Please analyze this plot.")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'df' is already loaded from the previous step in your Colab notebook
# If you are running this in a new Colab session, you'll need to re-upload and re-run the loading script first:
# file_name = 'Large_Industrial_Pump_Maintenance_Dataset.csv'
# df = pd.read_csv(file_name)


print("Generating Logistic Regression Plot for Maintenance Flag vs. Operational Hours...")

plt.figure(figsize=(10, 7)) # Adjust figure size for this single plot

# Create a logistic regression plot
# 'y' is the binary target (Maintenance_Flag), 'x' is the continuous feature (Operational_Hours)
# 'logistic=True' fits a logistic regression model and plots the probability curve
# 'scatter_kws' allows you to customize the scatter points (e.g., reduce transparency for clarity)
# 'line_kws' allows you to customize the line (e.g., color, linestyle)
sns.regplot(x='Operational_Hours', y='Maintenance_Flag', data=df, logistic=True,
            scatter_kws={'alpha':0.1}, # Adjust transparency of scatter points
            line_kws={'color': 'red'}) # Make the trend line clearly visible

plt.title('Probability of Maintenance Event vs. Operational Hours (Logistic Regression)')
plt.xlabel('Operational Hours')
plt.ylabel('Probability of Maintenance Flag = 1')
plt.grid(True) # Add a grid for better readability
plt.show()

print("\nLogistic regression trend plot generated successfully! Please analyze this plot.")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the CSV file into a Pandas DataFrame
file_name = 'Large_Industrial_Pump_Maintenance_Dataset.csv'  # This should work if the file is in Colab's session storage
try:
    df = pd.read_csv(file_name)
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found. Please ensure the CSV file is uploaded to your Colab environment's session storage.")
    exit()  # Exit the script if the file is not found

print("Generating Histogram of Operational Hours at Maintenance Events...")

# Create a new DataFrame containing only the rows where Maintenance_Flag is 1
maintenance_events = df[df['Maintenance_Flag'] == 1]

# Create the histogram
plt.figure(figsize=(10, 6))
sns.histplot(maintenance_events['Operational_Hours'], bins=30, kde=False, color='skyblue')  # Adjust bins as needed
plt.title('Distribution of Operational Hours at Maintenance Events')
plt.xlabel('Operational Hours')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)  # Add a grid for better readability
plt.show()

print("\nHistogram generated successfully! This shows the distribution of operational hours at which maintenance events occurred.")

import pandas as pd

# --- Data Loading (assuming you're running this in a fresh Colab cell or session) ---
# If 'df' is already loaded from previous steps, you can skip this block.
# Otherwise, make sure your CSV file is uploaded to Colab's session storage.
file_name = 'Large_Industrial_Pump_Maintenance_Dataset.csv'
try:
    df = pd.read_csv(file_name)
    print("Dataset loaded successfully for feature engineering.")
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found.")
    print("Please ensure the CSV file is uploaded to your Colab environment's session storage.")
    exit() # Exit if the file isn't found, as we can't proceed without it.
# --- End Data Loading Block ---


print("\nCreating new engineered features...")

# 1. Operational_Hours_Squared
df['Operational_Hours_Squared'] = df['Operational_Hours'] ** 2
print("Created 'Operational_Hours_Squared'")

# 2. Vibration_x_Temperature
df['Vibration_x_Temperature'] = df['Vibration'] * df['Temperature']
print("Created 'Vibration_x_Temperature'")

# 3. Pressure_to_Flow_Ratio
# We should add a check for Flow_Rate being zero to avoid division by zero errors
# Although for sensor data, Flow_Rate is unlikely to be exactly zero, it's good practice.
# We'll replace any resulting NaNs or Infs with a sensible value (e.g., 0 or the mean/median)
# For simplicity, let's just proceed, assuming Flow_Rate is non-zero, but note this as a potential future refinement.
df['Pressure_to_Flow_Ratio'] = df['Pressure'] / df['Flow_Rate']
print("Created 'Pressure_to_Flow_Ratio'")


print("\nFirst 5 rows of the DataFrame with new features:")
print(df.head())

print("\nDataFrame Info (to see new columns and their dtypes):")
df.info()

print("\nBasic Descriptive Statistics for new features:")
print(df[['Operational_Hours_Squared', 'Vibration_x_Temperature', 'Pressure_to_Flow_Ratio']].describe())

print("\nFeature engineering complete!")

import pandas as pd
from sklearn.model_selection import train_test_split

# --- Data Loading (essential for a fresh run in Colab) ---
# If 'df' is already loaded and feature-engineered from previous steps,
# you can skip this block. Otherwise, ensure your CSV file is uploaded.
file_name = 'Large_Industrial_Pump_Maintenance_Dataset.csv'
try:
    df = pd.read_csv(file_name)
    print("Dataset loaded successfully for data splitting.")
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found.")
    print("Please ensure the CSV file is uploaded to your Colab environment's session storage.")
    exit() # Exit if the file isn't found

# --- Feature Engineering (also essential if 'df' is newly loaded) ---
# Add the engineered features as discussed previously
df['Operational_Hours_Squared'] = df['Operational_Hours'] ** 2
df['Vibration_x_Temperature'] = df['Vibration'] * df['Temperature']
df['Pressure_to_Flow_Ratio'] = df['Pressure'] / df['Flow_Rate']
print("Engineered features added to the dataset.")
# --- End of Data Loading & Feature Engineering Block ---


print("\nPreparing data for modeling...")

# Define features (X) and target (y)
# We will drop 'Pump_ID' and 'Maintenance_Flag' from X
# 'Pump_ID' is currently a numerical identifier, not a measurement.
# We'll stick to the core sensor and engineered features for now.
features = ['Temperature', 'Vibration', 'Pressure', 'Flow_Rate', 'RPM',
            'Operational_Hours', 'Operational_Hours_Squared',
            'Vibration_x_Temperature', 'Pressure_to_Flow_Ratio']

X = df[features]
y = df['Maintenance_Flag']

print(f"Features (X) shape: {X.shape}")
print(f"Target (y) shape: {y.shape}")
print(f"Target distribution (Maintenance_Flag):\n{y.value_counts(normalize=True)}")


# Split the data into training and testing sets (50-50 split)
# stratify=y ensures that the proportion of classes in y is preserved in both train and test sets.
# random_state ensures reproducibility of the split.
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.5,
                                                    random_state=42,
                                                    stratify=y)

print("\nData splitting complete!")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

print(f"\ny_train distribution (Maintenance_Flag):\n{y_train.value_counts(normalize=True)}")
print(f"\ny_test distribution (Maintenance_Flag):\n{y_test.value_counts(normalize=True)}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# --- Data Loading and Feature Engineering Block (for full reproducibility) ---
# Ensure 'df' is available. If running in a new Colab cell, you'll need this.
file_name = 'Large_Industrial_Pump_Maintenance_Dataset.csv'
try:
    df = pd.read_csv(file_name)
    # print("Dataset loaded successfully for model training.") # Commented out to reduce verbose output if already run
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found.")
    print("Please ensure the CSV file is uploaded to your Colab environment's session storage.")
    exit()

df['Operational_Hours_Squared'] = df['Operational_Hours'] ** 2
df['Vibration_x_Temperature'] = df['Vibration'] * df['Temperature']
df['Pressure_to_Flow_Ratio'] = df['Pressure'] / df['Flow_Rate']
# print("Engineered features added to the dataset.") # Commented out

features = ['Temperature', 'Vibration', 'Pressure', 'Flow_Rate', 'RPM',
            'Operational_Hours', 'Operational_Hours_Squared',
            'Vibration_x_Temperature', 'Pressure_to_Flow_Ratio']

X = df[features]
y = df['Maintenance_Flag']

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.5,
                                                    random_state=42,
                                                    stratify=y)
# --- End of Data Loading & Feature Engineering Block ---


print("--- Starting Model Training ---")

# 1. Instantiate the RandomForestClassifier
# We'll use default parameters for the first run, except for random_state for reproducibility.
# n_estimators is the number of trees in the forest. Default is often 100.
# random_state ensures that results are consistent across multiple runs.
model = RandomForestClassifier(random_state=42)

print(f"Model initialized: {model}")

# 2. Train the model on the training data
print("Training the Random Forest Classifier...")
model.fit(X_train, y_train)

print("\nModel training complete!")

# You now have a trained 'model' object.
# The next step will be to use this model to make predictions and evaluate its performance.

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    classification_report,
    confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns

# --- Data Loading and Feature Engineering Block (for full reproducibility) ---
# Ensure 'df' is available. If running in a new Colab cell, you'll need this.
file_name = 'Large_Industrial_Pump_Maintenance_Dataset.csv'
try:
    df = pd.read_csv(file_name)
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found.")
    print("Please ensure the CSV file is uploaded to your Colab environment's session storage.")
    exit()

df['Operational_Hours_Squared'] = df['Operational_Hours'] ** 2
df['Vibration_x_Temperature'] = df['Vibration'] * df['Temperature']
df['Pressure_to_Flow_Ratio'] = df['Pressure'] / df['Flow_Rate']

features = ['Temperature', 'Vibration', 'Pressure', 'Flow_Rate', 'RPM',
            'Operational_Hours', 'Operational_Hours_Squared',
            'Vibration_x_Temperature', 'Pressure_to_Flow_Ratio']

X = df[features]
y = df['Maintenance_Flag']

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.5,
                                                    random_state=42,
                                                    stratify=y)

# --- Model Training Block (assuming it was run just before this) ---
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)
print("Model training confirmed before evaluation.")
# --- End of Model Training Block ---


print("\n--- Starting Model Evaluation ---")

# 1. Make predictions on the test set
y_pred = model.predict(X_test)
# For ROC AUC, we need predicted probabilities for the positive class (Maintenance_Flag = 1)
y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class (class 1)

print("\n--- Classification Metrics ---")

# 2. Calculate common classification metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba) # Use probabilities for ROC AUC

print(f"Accuracy:  {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1-Score:  {f1:.4f}")
print(f"ROC AUC:   {roc_auc:.4f}")

# 3. Generate a Classification Report
print("\n--- Classification Report ---")
print(classification_report(y_test, y_pred))

# 4. Generate and Visualize a Confusion Matrix
print("\n--- Confusion Matrix ---")
cm = confusion_matrix(y_test, y_pred)
print(cm) # Raw confusion matrix

# Visualize the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

print("\nModel evaluation complete!")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    classification_report,
    confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns

# --- Data Loading and Initial Feature Engineering Block (for full reproducibility) ---
# Ensure 'df' is available. If running in a new Colab cell, you'll need this.
file_name = 'Large_Industrial_Pump_Maintenance_Dataset.csv'
try:
    df = pd.read_csv(file_name)
    print("Dataset loaded successfully.")
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found.")
    print("Please ensure the CSV file is uploaded to your Colab environment's session storage.")
    exit()

# Add the previously engineered features
df['Operational_Hours_Squared'] = df['Operational_Hours'] ** 2
df['Vibration_x_Temperature'] = df['Vibration'] * df['Temperature']
df['Pressure_to_Flow_Ratio'] = df['Pressure'] / df['Flow_Rate']
print("Initial engineered features added.")
# --- End of Data Loading & Initial Feature Engineering Block ---


print("\n--- Generating Delta Features ---")

# 1. Sort the DataFrame by Pump_ID and then by Operational_Hours
# This is crucial for calculating differences correctly within each pump's sequence
df_sorted = df.sort_values(by=['Pump_ID', 'Operational_Hours']).copy()
print("DataFrame sorted by Pump_ID and Operational_Hours.")

# Define the sensor columns for which we want to calculate delta
sensor_cols = ['Temperature', 'Vibration', 'Pressure', 'Flow_Rate', 'RPM']

# 2. Calculate the delta for each sensor feature, grouped by Pump_ID
for col in sensor_cols:
    new_col_name = f"{col}_Delta"
    # Calculate the difference from the previous row *within each Pump_ID group*
    df_sorted[new_col_name] = df_sorted.groupby('Pump_ID')[col].diff()
    print(f"Calculated '{new_col_name}'")

# 3. Handle NaN values (first entry for each group after diff will be NaN)
# We will fill these NaNs with 0. This assumes that for the first record of a pump,
# there's no preceding change, or it's a baseline.
for col in sensor_cols:
    new_col_name = f"{col}_Delta"
    df_sorted[new_col_name] = df_sorted[new_col_name].fillna(0)
print("Filled NaN values in delta columns with 0.")

# Display the first few rows with the new delta features
print("\nFirst 10 rows of the sorted DataFrame with new delta features:")
# Displaying more rows and specific columns to clearly show the deltas
print(df_sorted[['Pump_ID', 'Operational_Hours'] + sensor_cols + [f"{c}_Delta" for c in sensor_cols]].head(10))

# Display info to check non-null counts for new features
print("\nDataFrame Info (checking non-null counts for new delta features):")
df_sorted.info()

print("\nDelta feature engineering complete!")

# Now, 'df_sorted' is our new DataFrame with the delta features.
# We will use this 'df_sorted' for the next modeling steps.

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    classification_report,
    confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns

# --- 1. Data Loading ---
file_name = 'Large_Industrial_Pump_Maintenance_Dataset.csv'
try:
    df = pd.read_csv(file_name)
    print("Dataset loaded successfully.")
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found.")
    print("Please ensure the CSV file is uploaded to your Colab environment's session storage.")
    exit()

# --- 2. Initial Feature Engineering ---
print("\nAdding initial engineered features...")
df['Operational_Hours_Squared'] = df['Operational_Hours'] ** 2
df['Vibration_x_Temperature'] = df['Vibration'] * df['Temperature']
df['Pressure_to_Flow_Ratio'] = df['Pressure'] / df['Flow_Rate']
print("Initial engineered features added.")

# --- 3. Delta Feature Engineering ---
print("\nGenerating Delta Features...")
# Sort the DataFrame by Pump_ID and then by Operational_Hours for correct delta calculation
df_sorted = df.sort_values(by=['Pump_ID', 'Operational_Hours']).copy()
print("DataFrame sorted by Pump_ID and Operational_Hours.")

sensor_cols = ['Temperature', 'Vibration', 'Pressure', 'Flow_Rate', 'RPM']
for col in sensor_cols:
    new_col_name = f"{col}_Delta"
    df_sorted[new_col_name] = df_sorted.groupby('Pump_ID')[col].diff()
    # Fill NaN values (first entry for each group) with 0
    df_sorted[new_col_name] = df_sorted[new_col_name].fillna(0)
    print(f"Calculated and handled NaNs for '{new_col_name}'")

print("Delta feature engineering complete. Using 'df_sorted' for modeling.")

# --- 4. Define Features (X) and Target (y) with new features ---
print("\nPreparing data for modeling with new features...")
# Updated list of features to include all original, initial engineered, and delta features
features = [
    'Temperature', 'Vibration', 'Pressure', 'Flow_Rate', 'RPM',
    'Operational_Hours', 'Operational_Hours_Squared',
    'Vibration_x_Temperature', 'Pressure_to_Flow_Ratio',
    'Temperature_Delta', 'Vibration_Delta', 'Pressure_Delta',
    'Flow_Rate_Delta', 'RPM_Delta'
]

X = df_sorted[features] # Use df_sorted here
y = df_sorted['Maintenance_Flag'] # Use df_sorted here

print(f"Features (X) shape: {X.shape}")
print(f"Target (y) shape: {y.shape}")
print(f"Target distribution (Maintenance_Flag):\n{y.value_counts(normalize=True)}")

# --- 5. Split the Data ---
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.5,
                                                    random_state=42,
                                                    stratify=y)
print("\nData splitting complete!")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")

# --- 6. Model Training ---
print("\n--- Starting Model Training ---")
model = RandomForestClassifier(random_state=42) # Using default parameters
print(f"Model initialized: {model}")
print("Training the Random Forest Classifier...")
model.fit(X_train, y_train)
print("Model training complete!")

# --- 7. Model Evaluation ---
print("\n--- Starting Model Evaluation ---")
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class (class 1)

print("\n--- Classification Metrics ---")
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

print(f"Accuracy:  {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1-Score:  {f1:.4f}")
print(f"ROC AUC:   {roc_auc:.4f}")

print("\n--- Classification Report ---")
print(classification_report(y_test, y_pred))

print("\n--- Confusion Matrix ---")
cm = confusion_matrix(y_test, y_pred)
print(cm)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

print("\nModel evaluation complete with new features!")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    classification_report,
    confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns

# --- 1. Data Loading ---
file_name = 'Large_Industrial_Pump_Maintenance_Dataset.csv'
try:
    df = pd.read_csv(file_name)
    print("Dataset loaded successfully.")
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found.")
    print("Please ensure the CSV file is uploaded to your Colab environment's session storage.")
    exit()

# --- 2. Initial Feature Engineering ---
print("\nAdding initial engineered features...")
df['Operational_Hours_Squared'] = df['Operational_Hours'] ** 2
df['Vibration_x_Temperature'] = df['Vibration'] * df['Temperature']
df['Pressure_to_Flow_Ratio'] = df['Pressure'] / df['Flow_Rate']
print("Initial engineered features added.")

# --- 3. Delta Feature Engineering ---
print("\nGenerating Delta Features...")
# Sort the DataFrame by Pump_ID and then by Operational_Hours for correct delta calculation
df_sorted = df.sort_values(by=['Pump_ID', 'Operational_Hours']).copy()
print("DataFrame sorted by Pump_ID and Operational_Hours.")

sensor_cols = ['Temperature', 'Vibration', 'Pressure', 'Flow_Rate', 'RPM']
for col in sensor_cols:
    new_col_name = f"{col}_Delta"
    df_sorted[new_col_name] = df_sorted.groupby('Pump_ID')[col].diff()
    df_sorted[new_col_name] = df_sorted[new_col_name].fillna(0) # Fill NaNs for the first record of each group
    print(f"Calculated and handled NaNs for '{new_col_name}'")

print("Delta feature engineering complete.")


# --- 4. Pump-Specific Z-Score Features ---
print("\nGenerating Pump-Specific Z-Score Features...")

# Columns for which to calculate pump-specific Z-scores
# We'll use the original sensor readings, not the deltas for this.
cols_for_zscore = ['Temperature', 'Vibration', 'Pressure', 'Flow_Rate', 'RPM', 'Operational_Hours']

for col in cols_for_zscore:
    # Calculate pump-specific mean and standard deviation
    pump_mean = df_sorted.groupby('Pump_ID')[col].transform('mean')
    pump_std = df_sorted.groupby('Pump_ID')[col].transform('std')

    # Calculate Z-score: (value - mean) / std
    new_zscore_col_name = f"{col}_ZScore_Pump"
    df_sorted[new_zscore_col_name] = (df_sorted[col] - pump_mean) / pump_std

    # Handle potential NaNs in Z-score (e.g., if std is 0 for a group, or if a pump only has one record)
    # Filling with 0 implies no deviation from the (single) mean, or no meaningful deviation.
    df_sorted[new_zscore_col_name] = df_sorted[new_zscore_col_name].fillna(0)
    print(f"Calculated and handled NaNs for '{new_zscore_col_name}'")

print("Pump-specific Z-score feature engineering complete.")

# Display first few rows with all new features and info
print("\nFirst 5 rows of DataFrame with all engineered features:")
print(df_sorted.head())
print("\nDataFrame Info (checking non-null counts for new Z-score features):")
df_sorted.info()


# --- 5. Define Features (X) and Target (y) with ALL new features ---
print("\nPreparing data for modeling with ALL new features...")
features = [
    'Temperature', 'Vibration', 'Pressure', 'Flow_Rate', 'RPM',
    'Operational_Hours',
    'Operational_Hours_Squared',
    'Vibration_x_Temperature', 'Pressure_to_Flow_Ratio',
    'Temperature_Delta', 'Vibration_Delta', 'Pressure_Delta',
    'Flow_Rate_Delta', 'RPM_Delta',
    'Temperature_ZScore_Pump', 'Vibration_ZScore_Pump', 'Pressure_ZScore_Pump',
    'Flow_Rate_ZScore_Pump', 'RPM_ZScore_Pump', 'Operational_Hours_ZScore_Pump'
]

X = df_sorted[features]
y = df_sorted['Maintenance_Flag']

print(f"Features (X) shape: {X.shape}")
print(f"Target (y) shape: {y.shape}")

# --- 6. Split the Data ---
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.5,
                                                    random_state=42,
                                                    stratify=y)
print("\nData splitting complete!")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")

# --- 7. Model Training ---
print("\n--- Starting Model Training ---")
model = RandomForestClassifier(random_state=42) # Using default parameters
print(f"Model initialized: {model}")
print("Training the Random Forest Classifier...")
model.fit(X_train, y_train)
print("Model training complete!")

# --- 8. Model Evaluation ---
print("\n--- Starting Model Evaluation ---")
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1] # Probability of the positive class (class 1)

print("\n--- Classification Metrics ---")
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

print(f"Accuracy:  {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1-Score:  {f1:.4f}")
print(f"ROC AUC:   {roc_auc:.4f}")

print("\n--- Classification Report ---")
print(classification_report(y_test, y_pred))

print("\n--- Confusion Matrix ---")
cm = confusion_matrix(y_test, y_pred)
print(cm)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

print("\nModel evaluation complete with all engineered features!")

